Our results showed how to use nonlocal theory to approximate the derivative with respect to unknowable functions.

Then we applied this theory to solve differential equations.

Furthermore, nowadays in the Natural Language Processing (NLP) field, there are lots of derivative-free situations (DALL $\cdot$ E, GPT-4, GPT-4o, etc.), 
which means that we cannot get the true gradient from the APIs except for the object functionâ€™s value. 
So how to optimize the loss function in the black box tuning situation becomes more intractable. 
We want to use the nonlocal derivative to estimate the true derivative (1-dimensional) which gives us a new perspective on how to take cognitive derivatives. 
Zeroth order method in black-box optimization is really useful and the main idea is using a \textbf{Gaussian kernel} to approximate gradient.
